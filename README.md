# Option Pricing with Numerical Methods and Machine Learning

This project implements and compares several approaches for pricing European call options under the Blackâ€“Scholes framework:

- **Analytical solution** (closed-form Blackâ€“Scholes formula)
- **Monte Carlo (MC)** simulation
- **Quasiâ€“Monte Carlo (QMC)** with Sobol sequences
- **Finite Difference (FD)** PDE solvers:
  - Explicit
  - Implicit
  - Crankâ€“Nicolson
- **Machine Learning Surrogate Model** (PyTorch neural network trained on QMC-generated data)

---
## ğŸ“‚ Project Structure

```
option_pricing_project/
â”œâ”€â”€ data/                         # Dataset
â”‚   â””â”€â”€ synthetic_qmc.csv         # Generated training/validation dataset
â”‚
â”œâ”€â”€ src/                          # Python source code
â”‚   â”œâ”€â”€ monte_carlo.py            # MC & QMC simulation functions
â”‚   â”œâ”€â”€ finite_difference.py      # Explicit, implicit, CN schemes
â”‚   â”œâ”€â”€ black_scholes.py          # Closed-form formulas
â”‚   â”œâ”€â”€ dataset.py                # Data generation + CSV export
â”‚   â”œâ”€â”€ surrogate_model.py        # PyTorch model + training loop
â”‚   â””â”€â”€ evaluation.py             # Convergence, runtime, error plots
â”‚
â”œâ”€â”€ results/                      # Generated plots & reports
â”‚   â”œâ”€â”€ convergence_plot.png
â”‚   â”œâ”€â”€ efficiency_comparison.png
â”‚   â”œâ”€â”€ fd_convergence.png
â”‚   â””â”€â”€ evaluation_results.csv  
â”‚
â”œâ”€â”€ requirements.txt              # Dependencies
â”œâ”€â”€ README.md                     # Project overview, setup & usage
â””â”€â”€ main.py                       # Entry point (run full pipeline end-to-end)
```
---

## âš™ï¸ Installation

```bash
git clone <your-repo>
cd option_pricing_project
pip install -r requirements.txt
```
Dependencies include:
* `numpy`, `scipy`, `pandas`, `matplotlib`
* `torch`, `scikit-learn`
---

## â–¶ï¸ Usage

Run the full pipeline end-to-end:

```bash
python3 main.py
```
This will:
1. Generate QMC training data and save to `data/synthetic_qmc.csv`.
2. Train the surrogate neural network.
3. Run MC, QMC, FD, and surrogate pricing comparisons.
4. Save benchmark plots and results into `results/`.

You can also run individual components:
* **QMC dataset generation**
```bash
python3 src/dataset.py
```
* **Finite difference test**
```bash
python3 src/finite_difference.py
```

* **Surrogate model training**
```bash
python3 src/surrogate_model.py
```

## ğŸ“Š Outputs
All results are stored in the `results/` folder:
* `convergence_plot.png` - MC vs QMC convergence (error vs samples).
* `efficiency_comparison.png` - Runtime vs accuracy (all methods).
* `fd_convergence.png` - Benchmark convergence/runtime data.
* `evaluation_results.csv` - Benchmark convergence/runtime data.

## Results
The project benchmarks analytical, simulation, PDE, and ML approaches for European call option pricing.

### Monte Carlo vs Quasiâ€“Monte Carlo
* MC: Converges at the theoretical $O(1/\sqrt{N})$ rate; suffers from high variance.
* QMC (Sobol): Substantially more accurate than MC at the same sample size, reducing variance by more than an order of magnitude for small $N$.
* Key finding: QMC consistently delivered the highest accuracy among stochastic simulation methods.

### Finite Difference (FD) Methods
* Explicit: Conditionally stable; slow convergence; consistent with first-order accuracy.
* Implicit: Unconditionally stable, with results often close to Blackâ€“Scholes at moderate grids.
* Crankâ€“Nicolson (CN): Achieved second-order convergence and was the most accurate PDE scheme at refined grids.
* Observation: At coarse grids, implicit sometimes matched Blackâ€“Scholes better due to error cancellation, but CN dominated at higher resolution.

### Machine Learning Surrogate
* Neural network surrogate trained on QMC-generated data, achieving <0.5% relative error compared to Blackâ€“Scholes.
* Strength: Once trained, inference is effectively instantaneous, making it suitable for real-time or large-scale option pricing.
* Trade-off: Training required more time than simulation, but runtime efficiency surpassed all other methods once deployed.

### Efficiency Comparison
* MC: Slowest convergence and highest variance.
* QMC: Faster convergence with competitive runtimes.
* FD: More efficient than simulations, especially Crankâ€“Nicolson on fine grids.
* ML Surrogate: Highest upfront training cost, but inference is fastest in production.
* Efficiency Frontier: A combination of Surrogate + CN offers the best practical trade-off depending on whether training is precomputed.
